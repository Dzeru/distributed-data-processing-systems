# Scala

## Исходный файл

Данные для обработки содержатся в файле _agg_symbols.csv_.

```
load_date,symbol,open_price,highest_price,lowest_price,close_price
2021-12-10 13:00,A,154,154,154,154
2021-12-10 17:00,A,154.53,155.7,154.08,154.87
2021-12-10 21:00,A,154.9,156.34,154.72,156.23
2021-12-11 01:00,A,156.26,156.42,156.26,156.42
2021-12-10 13:00,AA,50.26,50.26,50.02,50.1
2021-12-10 17:00,AA,50.14,50.47,47.84,48.555
2021-12-10 21:00,AA,48.52,48.87,48.3799,48.78
2021-12-11 01:00,AA,48.8,48.9,48.46,48.76
```

* `load_date` - дата и время выгрузки данных;
* `symbol` - название акции;
* `open_price` - цена открытия периода;
* `highest_price` - наибольшая цена за период;
* `lowest_price` - наименьшая цена за период;
* `close_price` - цена закрытия периода.

## Перед началом работы

Сборка проекта производится с помощью sbt.

Указываются название и версия проекта.
Указывается используемая версия языка Scala.

Далее подключаются 2 зависимости: Spark Core и Spark SQL версии 3.2.0.

```scala
name := "spark_example_scala"
version := "1.0.0"
scalaVersion := "2.12.12"

libraryDependencies ++= Seq(
  ("org.apache.spark" %% "spark-core" % "3.2.0"),
  ("org.apache.spark" %% "spark-sql" % "3.2.0")
)
```

## Разбор кода

В каком пакете лежит код (аналогично Java).

```scala
package com.dzeru.sparkexamplescala
```

Импортируем нужные пакеты для работы с SQL, функциями и типами Spark.

```scala
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
```

Объявляем класс, в котором будет описан код программы.

```scala
object Main
```

С помощью ленивой инициализации создаем объект сессии Spark, который будет обрабатывать данные.

```scala
@transient lazy val spark: SparkSession = SparkSession
.builder()
```

Так будет называться приложение.

```scala
.appName("spark_example_scala")
```

Данная версия запускается локально.

```scala
.master("local[*]")
```

Если сессия существует, возьмется созданная версия, иначе создастся новая.

```scala
.getOrCreate()
```

В итоге формируется следующий объект:

```scala
@transient lazy val spark: SparkSession = SparkSession
  .builder()
  .appName("spark_example_scala")
  .master("local[*]")
  .getOrCreate()
```

Импорт ```implicits``` позволит нам использовать более лаконичный синтаксис и преобразовывать типы скрытно. 
Например, использовать ```$"name"``` вместо ```col(name)```.

```scala
import spark.implicits._
```

Функция, в которой будет описана основная логика. 
Кто знаком с Java, может узнать в ней аналог функции main.

```scala
def main(args: Array[String]): Unit 
```

Задаем путь к входному файлу. В данном случае путь передается как аргумент консоли.

```scala
val path = args(0)
```

Считываем файл в формате CSV по заданному пути. 
Учитываем, что у файла есть заголовок, а escape задан двойными кавычками. 
Файл считывается в объект класса ```DataFrame```. 
Это представление похоже на реляционную таблицу, которая лежит в распределенной памяти кластера.

```scala
val aggSymbolsReadDF: DataFrame = spark.read.options(Map("header" -> "true", "escape" -> "\"")).csv(path)
```

Выводим в консоль 8 первых считанных строк.

```scala
aggSymbolsReadDF.show(8)
```

Вывод в консоль:

```
+----------------+------+----------+-------------+------------+-----------+
|       load_date|symbol|open_price|highest_price|lowest_price|close_price|
+----------------+------+----------+-------------+------------+-----------+
|2021-12-10 13:00|     A|       154|          154|         154|        154|
|2021-12-10 17:00|     A|    154.53|        155.7|      154.08|     154.87|
|2021-12-10 21:00|     A|     154.9|       156.34|      154.72|     156.23|
|2021-12-11 01:00|     A|    156.26|       156.42|      156.26|     156.42|
|2021-12-10 13:00|    AA|     50.26|        50.26|       50.02|       50.1|
|2021-12-10 17:00|    AA|     50.14|        50.47|       47.84|     48.555|
|2021-12-10 21:00|    AA|     48.52|        48.87|     48.3799|      48.78|
|2021-12-11 01:00|    AA|      48.8|         48.9|       48.46|      48.76|
+----------------+------+----------+-------------+------------+-----------+
```

Редактируем считанный датасет. Для этого необходимо сделать селект-запрос к его полям.

```scala
val aggSymbolsCastDF: DataFrame = aggSymbolsReadDF.select(...)
```

Выбираем колонку ```load_date``` и преобразуем ее из строки в timestamp. 
После преобразования можем переименовать колонку с помощью функции ```as```.

```scala
$"load_date".cast(TimestampType).as("loadDate")
```

Стоит отметить, что колонка выбирается с помощью символа ```$```.
Этот синтаксис доступен, потому что ранее был подключен модуль ```implicits``` Spark.  
Также можно обращаться к колонкам с помощью ```col(columnname)``` и не только.

Также в запросе выбираем колонку ```open_price``` и преобразовываем ее в тип decimal, 
содержащий 12 символов всего и 4 в дробной части. Колонка также переименована.

```scala
$"open_price".cast(DecimalType(12, 4)).as("openPrice")
```

Аналогично преобразовываем остальные колонки. В итоге получается следующий селект-запрос:

```scala
val aggSymbolsCastDF: DataFrame = aggSymbolsReadDF.select(
      $"load_date".cast(TimestampType).as("loadDate"),
      $"symbol",
      $"open_price".cast(DecimalType(12, 4)).as("openPrice"),
      $"highest_price".cast(DecimalType(12, 4)).as("highestPrice"),
      $"lowest_price".cast(DecimalType(12, 4)).as("lowestPrice"),
      $"close_price".cast(DecimalType(12, 4)).as("closePrice")
    )
```

Вывод в консоль:

```
+-------------------+------+---------+------------+-----------+----------+
|           loadDate|symbol|openPrice|highestPrice|lowestPrice|closePrice|
+-------------------+------+---------+------------+-----------+----------+
|2021-12-10 13:00:00|     A| 154.0000|    154.0000|   154.0000|  154.0000|
|2021-12-10 17:00:00|     A| 154.5300|    155.7000|   154.0800|  154.8700|
|2021-12-10 21:00:00|     A| 154.9000|    156.3400|   154.7200|  156.2300|
|2021-12-11 01:00:00|     A| 156.2600|    156.4200|   156.2600|  156.4200|
|2021-12-10 13:00:00|    AA|  50.2600|     50.2600|    50.0200|   50.1000|
|2021-12-10 17:00:00|    AA|  50.1400|     50.4700|    47.8400|   48.5550|
|2021-12-10 21:00:00|    AA|  48.5200|     48.8700|    48.3799|   48.7800|
|2021-12-11 01:00:00|    AA|  48.8000|     48.9000|    48.4600|   48.7600|
+-------------------+------+---------+------------+-----------+----------+
```

Далее будет вычислено среднее значение наименьших цен относительно акций ```symbol``` и даты ```loadDate```.

Будет создан ```DataFrame``` из селект-запроса.
Выбирается только 3 поля: ```loadDate```, ```symbol``` и ```lowestPrice```.
Данные группируются по названию акций.
Далее вызывается функция ```agg```, которая принимает одну из конкретных функций агрегации, 
в данном случае это функция поиска среднего значения ```avg``` по колонке ```lowestPrice```.
Для удобства тоже переименуем эту колонку.
Данные будут выводиться в убывающем порядке относительно колонки ```symbol```.

Стоит отметить, что, несмотря на 3 выбранных поля, в результате выведется только 2, 
потому что поле ```loadDate``` не встречается ни в функции группировки, ни в функции агрегации.

```scala
val avgLowestPricesDF: DataFrame = aggSymbolsCastDF
  .select($"loadDate", $"symbol", $"lowestPrice")
  .groupBy($"symbol")
  .agg(avg("lowestPrice").as("avgLowestPrice"))
  .orderBy(desc("symbol"))
```

Вывод в консоль:

```
+------+--------------+
|symbol|avgLowestPrice|
+------+--------------+
|    AA|   48.67497500|
|     A|  154.76500000|
+------+--------------+
```

Чтобы выводились все 3 поля, добавим в функцию группировки колонку ```loadDate```.
Однако в исходных данных эта колонка содержит уникальные и дату, и время, что сводит на нет смысл группировки.
Для этого сконвертируем ее из timestamp в date, чтобы оставить только дату. 
В остальном запрос аналогичен предыдущему.

```scala
val avgLowestPriceByDayDF: DataFrame = aggSymbolsCastDF
  .select($"loadDate", $"symbol", $"lowestPrice")
  .groupBy($"symbol", $"loadDate".cast(DateType).as("loadDate"))
  .agg(avg("lowestPrice").as("avgLowestPrice"))
  .orderBy(desc("symbol"))
```

Вывод в консоль:

```
+------+----------+--------------+
|symbol|  loadDate|avgLowestPrice|
+------+----------+--------------+
|    AA|2021-12-11|   48.46000000|
|    AA|2021-12-10|   48.74663333|
|     A|2021-12-10|  154.26666667|
|     A|2021-12-11|  156.26000000|
+------+----------+--------------+
```